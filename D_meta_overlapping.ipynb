{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZxACd4s8qJT",
        "outputId": "c83a4bef-f60f-46e5-8771-4e47addca239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Check and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18s4Qd6g8tzu"
      },
      "outputs": [],
      "source": [
        "# Federated learning hyperparameters\n",
        "num_epochs, learning_rate = 5, 0.001\n",
        "batch_sizes, communication_rounds = [64, 128], 100\n",
        "overlap_percentage = 25 # Dmeta hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fJ-ueVW087QW"
      },
      "outputs": [],
      "source": [
        "# Define a deep learning model (CNN)\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Modify the architecture based on your requirements\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 10)  # Assuming 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FziDvqIm_AG",
        "outputId": "ed8bc5a3-a90f-45e4-9227-58faa33b9746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f4569b2c2544bd597c567acd9f8a49d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/26421880 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting path_to_client_data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "830667b482434287956e3f9e31876fc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/29515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting path_to_client_data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07d9d0169e8f4eb4afc123c66198a417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4422102 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting path_to_client_data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0685b237a27542898167773e399e3f47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting path_to_client_data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to path_to_client_data\\FashionMNIST\\raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Dataset preparation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Assuming you have the FEMNIST dataset downloaded and split for each client\n",
        "# Replace 'path_to_client_data' with the actual path to the client data\n",
        "client_dataset = torchvision.datasets.FashionMNIST(root='path_to_client_data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the client dataset into train and validation sets\n",
        "train_size = int(0.8 * len(client_dataset))\n",
        "val_size = len(client_dataset) - train_size\n",
        "client_train_set, client_val_set = random_split(client_dataset, [train_size, val_size])\n",
        "\n",
        "# DataLoader for training and validation\n",
        "train_loaders = [DataLoader(client_train_set, batch_size=batch_size, shuffle=True, num_workers=4) for batch_size in batch_sizes]\n",
        "val_loader = DataLoader(client_val_set, batch_size=batch_sizes[0], shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JcHYTj6t9C62"
      },
      "outputs": [],
      "source": [
        "# Function to train the model for a given federated learning algorithm\n",
        "def train_federated_learning_with_dmeta(model, train_loader, dmeta_loader, algorithm_name):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    performance_metrics = {'accuracy': [], 'loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train on D\n",
        "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - {algorithm_name} - D'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Train on Dmeta\n",
        "        for images, labels in tqdm(dmeta_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - {algorithm_name} - Dmeta'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print average loss at the end of each epoch\n",
        "        average_loss = running_loss / (len(train_loader) + len(dmeta_loader))\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        accuracy = evaluate_model(model, val_loader)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "        performance_metrics['accuracy'].append(accuracy)\n",
        "        performance_metrics['loss'].append(average_loss)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Function to evaluate the model on the validation set\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def train_federated_learningAdj(model, train_loader, algorithm_name, times):\n",
        "#     model.train()\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "#     # Initialize the convolutional layer outside the loop\n",
        "#     in_channels = train_loader.dataset[0][0].shape[0]  # Number of input channels\n",
        "#     out_channels = max(1, in_channels - times) \n",
        "#     conv_reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    \n",
        "#     performance_metrics = {'accuracy': [], 'loss': []}\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         running_loss = 0.0\n",
        "#         for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - {algorithm_name}'):\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "#             # Apply the convolutional layer to input images\n",
        "#             data = conv_reduce_channels(images)\n",
        "            \n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(data)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#         # Print average loss at the end of each epoch\n",
        "#         average_loss = running_loss / len(train_loader)\n",
        "#         print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Loss: {average_loss:.4f}')\n",
        "\n",
        "#         # Evaluate on validation set\n",
        "#         accuracy = evaluate_model(model, val_loader)\n",
        "#         print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "#         performance_metrics['accuracy'].append(accuracy)\n",
        "#         performance_metrics['loss'].append(average_loss)\n",
        "\n",
        "#     return performance_metrics\n",
        "\n",
        "# Function to train the model for a given federated learning algorithm\n",
        "def train_federated_learning_with_dmetaAdj(model, train_loader, dmeta_loader, algorithm_name, times):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "     # Initialize the convolutional layer outside the loop\n",
        "    in_channels = train_loader.dataset[0][0].shape[0]  # Number of input channels\n",
        "    out_channels = max(1, in_channels - times) \n",
        "    conv_reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    performance_metrics = {'accuracy': [], 'loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train on D\n",
        "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - {algorithm_name} - D'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            data = conv_reduce_channels(images)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Train on Dmeta\n",
        "        for images, labels in tqdm(dmeta_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - {algorithm_name} - Dmeta'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            data = conv_reduce_channels(images)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print average loss at the end of each epoch\n",
        "        average_loss = running_loss / (len(train_loader) + len(dmeta_loader))\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Loss: {average_loss:.4f}')\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        accuracy = evaluate_model(model, val_loader)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}] - {algorithm_name}, Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "        performance_metrics['accuracy'].append(accuracy)\n",
        "        performance_metrics['loss'].append(average_loss)\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Function to evaluate the model on the validation set\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BXxeytPknmPY"
      },
      "outputs": [],
      "source": [
        "# Number of writers in Dmeta\n",
        "num_writers_dmeta = int(overlap_percentage / 100 * len(client_dataset))\n",
        "\n",
        "# Select writers from D and the auxiliary dataset\n",
        "writers_d = client_dataset.targets.unique().tolist()\n",
        "writers_auxiliary = list(range(100))  # Assuming 100 writers in the auxiliary dataset\n",
        "\n",
        "# Select a certain proportion of writers from D and the rest from the auxiliary dataset\n",
        "selected_writers_d = writers_d[:len(writers_d) - num_writers_dmeta]\n",
        "selected_writers_auxiliary = writers_auxiliary[:num_writers_dmeta]\n",
        "\n",
        "# Combine selected writers to form Dmeta writers\n",
        "writers_dmeta = selected_writers_d + selected_writers_auxiliary\n",
        "\n",
        "# Sample 1% examples to form Dmeta\n",
        "num_samples_dmeta = int(len(client_dataset))\n",
        "indices_dmeta = torch.randperm(len(client_dataset))[:num_samples_dmeta]\n",
        "\n",
        "# Create Dmeta by selecting examples from D and auxiliary dataset\n",
        "data_dmeta = [client_dataset[i] for i in indices_dmeta]\n",
        "dmeta_loader = DataLoader(data_dmeta, batch_size=batch_sizes[0], shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGL7i3wtzFNV",
        "outputId": "c30142eb-46f1-4411-fd48-0d4b14688223"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta - D:   0%|          | 0/750 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta - D: 100%|██████████| 750/750 [00:36<00:00, 20.74it/s]\n",
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta - Dmeta: 100%|██████████| 938/938 [03:00<00:00,  5.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta, Loss: 0.3344\n",
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta, Validation Accuracy: 92.25%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - FedMeta w/ UGA with Dmeta - D: 100%|██████████| 750/750 [01:49<00:00,  6.88it/s]\n",
            "Epoch 2/5 - FedMeta w/ UGA with Dmeta - Dmeta: 100%|██████████| 938/938 [01:37<00:00,  9.63it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta, Loss: 0.1959\n",
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta, Validation Accuracy: 94.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - FedMeta w/ UGA with Dmeta - D: 100%|██████████| 750/750 [00:47<00:00, 15.84it/s]\n",
            "Epoch 3/5 - FedMeta w/ UGA with Dmeta - Dmeta: 100%|██████████| 938/938 [01:10<00:00, 13.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta, Loss: 0.1352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta, Validation Accuracy: 96.08%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - FedMeta w/ UGA with Dmeta - D: 100%|██████████| 750/750 [00:47<00:00, 15.88it/s]\n",
            "Epoch 4/5 - FedMeta w/ UGA with Dmeta - Dmeta: 100%|██████████| 938/938 [01:11<00:00, 13.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta, Loss: 0.0881\n",
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta, Validation Accuracy: 96.99%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - FedMeta w/ UGA with Dmeta - D: 100%|██████████| 750/750 [00:48<00:00, 15.58it/s]\n",
            "Epoch 5/5 - FedMeta w/ UGA with Dmeta - Dmeta: 100%|██████████| 938/938 [02:18<00:00,  6.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta, Loss: 0.0566\n",
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta, Validation Accuracy: 98.41%\n"
          ]
        }
      ],
      "source": [
        "fedmeta_model = CNNModel().to(device)\n",
        "# Train & Evaluate FedMeta w/ UGA\n",
        "fedmeta_uga_performance_dmeta = train_federated_learning_with_dmeta(fedmeta_model, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta:   0%|          | 0/938 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000029ADF054700>\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"c:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
            "    if self._persistent_workers or self._workers_status[worker_id]:\n",
            "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [02:16<00:00,  6.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Loss: 0.2144\n",
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Validation Accuracy: 80.03%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [03:45<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Loss: 0.1378\n",
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Validation Accuracy: 82.66%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [02:12<00:00,  7.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Loss: 0.1138\n",
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Validation Accuracy: 85.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [04:26<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Loss: 0.0957\n",
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Validation Accuracy: 88.22%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - FedMeta w/ UGA with Dmeta(50 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [03:12<00:00,  4.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Loss: 0.0787\n",
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta(50 Removed Nodes), Validation Accuracy: 88.63%\n"
          ]
        }
      ],
      "source": [
        "fedmeta_model1 = CNNModel().to(device)\n",
        "fedmeta_uga_performance_dmeta1 = train_federated_learning_with_dmetaAdj(fedmeta_model1, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta(50 Removed Nodes)', 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta(100 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [01:29<00:00, 10.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Loss: 0.2872\n",
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Validation Accuracy: 9.12%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - FedMeta w/ UGA with Dmeta(100 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [02:44<00:00,  5.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Loss: 0.1812\n",
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Validation Accuracy: 19.40%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - FedMeta w/ UGA with Dmeta(100 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [02:07<00:00,  7.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Loss: 0.1529\n",
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Validation Accuracy: 23.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - FedMeta w/ UGA with Dmeta(100 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [01:13<00:00, 12.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Loss: 0.1360\n",
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Validation Accuracy: 19.38%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - FedMeta w/ UGA with Dmeta(100 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [01:58<00:00,  7.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Loss: 0.1234\n",
            "Epoch [5/5] - FedMeta w/ UGA with Dmeta(100 Removed Nodes), Validation Accuracy: 27.45%\n"
          ]
        }
      ],
      "source": [
        "fedmeta_model2 = CNNModel().to(device)\n",
        "fedmeta_uga_performance_dmeta2 = train_federated_learning_with_dmetaAdj(fedmeta_model2, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta(100 Removed Nodes)', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - FedMeta w/ UGA with Dmeta(200 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [01:35<00:00,  9.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Loss: 0.2433\n",
            "Epoch [1/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Validation Accuracy: 77.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 - FedMeta w/ UGA with Dmeta(200 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [03:55<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Loss: 0.1540\n",
            "Epoch [2/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Validation Accuracy: 78.96%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 - FedMeta w/ UGA with Dmeta(200 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [03:14<00:00,  4.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Loss: 0.1294\n",
            "Epoch [3/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Validation Accuracy: 78.88%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 - FedMeta w/ UGA with Dmeta(200 Removed Nodes) - Dmeta: 100%|██████████| 938/938 [01:55<00:00,  8.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Loss: 0.1097\n",
            "Epoch [4/5] - FedMeta w/ UGA with Dmeta(200 Removed Nodes), Validation Accuracy: 78.13%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 - FedMeta w/ UGA with Dmeta(200 Removed Nodes) - Dmeta:  98%|█████████▊| 923/938 [01:18<00:01, 11.72it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fedmeta_model3 \u001b[38;5;241m=\u001b[39m CNNModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m fedmeta_uga_performance_dmeta3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_federated_learning_with_dmetaAdj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfedmeta_model3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmeta_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFedMeta w/ UGA with Dmeta(200 Removed Nodes)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[7], line 67\u001b[0m, in \u001b[0;36mtrain_federated_learning_with_dmetaAdj\u001b[1;34m(model, train_loader, dmeta_loader, algorithm_name, times)\u001b[0m\n\u001b[0;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     66\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 67\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Print average loss at the end of each epoch\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\vinji\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:412\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 412\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "fedmeta_model3 = CNNModel().to(device)\n",
        "fedmeta_uga_performance_dmeta3 = train_federated_learning_with_dmetaAdj(fedmeta_model3, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta(200 Removed Nodes)', 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fedmeta_model4 = CNNModel().to(device)\n",
        "fedmeta_uga_performance_dmeta4 = train_federated_learning_with_dmetaAdj(fedmeta_model4, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta(400 Removed Nodes)', 400)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fedmeta_model5 = CNNModel().to(device)\n",
        "fedmeta_uga_performance_dmeta5 = train_federated_learning_with_dmetaAdj(fedmeta_model5, train_loaders[0], dmeta_loader, 'FedMeta w/ UGA with Dmeta(800 Removed Nodes)', 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots for FEMNIST\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Test accuracy comparison\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title(\"Accuracy Comparison\")\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta['accuracy'], label='FedMeta w/ UGA')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta1['accuracy'], label='FedMeta w/ UGA(50 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta2['accuracy'], label='FedMeta w/ UGA(100 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta3['accuracy'], label='FedMeta w/ UGA(200 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta4['accuracy'], label='FedMeta w/ UGA(400 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta5['accuracy'], label='FedMeta w/ UGA(800 Removed)')\n",
        "\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title(\"Loss Comparison\")\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta['loss'], label='FedMeta w/ UGA')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta1['loss'], label='edMeta w/ UGA(50 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta2['loss'], label='edMeta w/ UGA(100 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta3['loss'], label='FedMeta w/ UGA(200 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta4['loss'], label='FedMeta w/ UGA(400 Removed)')\n",
        "sns.lineplot(x=range(0, communication_rounds, 20), y=fedmeta_uga_performance_dmeta5['loss'], label='FedMeta w/ UGA(800 Removed)')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
